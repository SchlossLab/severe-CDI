---
title: "Predicting _C. difficile_ Infection Severity from the Taxonomic Composition of the Gut Microbiome"
bibliography: references.bib
fontsize: 11pt
date: last-modified
date-format: medium
prefer-html: true
format:
  manuscript-pdf:
    mainfont: Helvetica
    keep-tex: true
    linenumbers: true
    doublespacing: true
    lof: false
  html:
    embed-resources: true
runningtitle: Predicting CDI Severity from OTUs
runningauthor: Sovacool
papertype: Preprint
author:
  - name: Kelly L. Sovacool
    orcid: 0000-0003-3283-829X
    affiliations:
      - ref: bioinf
  - name: Sarah E. Tomkovich
    affiliations:
      - ref: micro
  - name: Megan L. Coden
    affiliations:
      - ref: mcdb
  - name: Vincent B. Young
    affiliations:
      - ref: micro
      - ref: intmed
  - name: Krishna Rao
    affiliations:
      - ref: intmed
  - name: Patrick D. Schloss
    orcid: 0000-0002-6935-4275
    email: pschloss@umich.edu
    corresponding: true
    affiliations:
      - ref: micro
      - ref: ccmb
affiliations:
  - id: bioinf
    name: Department of Computational Medicine & Bioinformatics, University of Michigan
  - id: micro
    name: Department of Microbiology & Immunology, University of Michigan
  - id: mcdb
    name: Department of Molecular, Cellular, and Developmental Biology, University of Michigan
  - id: intmed
    name: Division of Infectious Diseases, Department of Internal Medicine, University of Michigan
  - id: ccmb
    name: Center for Computational Medicine and Bioinformatics, University of Michigan
#abstract: TODO
#importance: TODO
keywords: _C. difficile_ infection, supervised machine learning, gut microbiome, amplicion sequencing
---
```{r setup, eval=TRUE, echo=FALSE, cache=FALSE}
schtools::set_knitr_opts()
knitr::opts_chunk$set(dpi = 600)
```
```{r load_data, message = FALSE}
library(assertthat)
library(here)
library(kableExtra)
library(knitr)
library(tidyverse)

load(here("results", "stats.RData"))
table_font_size <- 10
```

# Introduction

prevalence of cdi. prevalence of severe cdi outcomes.

Numerous studies indicate that the gut microbiome may play a role in _C. diff_ 
colonization, infection, and clearance.
Contribution of the gut microbiome. 

prediction models based on EHR for whether infection occurs in the first place already in use.
so how about predicting severity of infections to guide treatment?
also models on EHR to predict adverse outcomes (Li, Rao).
OTUs vs EHRs to predict severity.
CDI severity prediction models could be deployed to screen patients at risk and
guide clinicians to consider prescribing a different course of treatment.
When paired with treatment options that may reduce risk of severity, deploying
prediction models can guide clinician decision-making to improve patient outcomes
while minimizing unnecessary harms.

A few ways to define CDI severity (@fig-flowchart).

The IDSA definition is known to be a poor predictor of adverse outcomes [@stevens_validation_2020],
however, it is easy to collect.

new dataset. 

Two goals: investigate whether we can predict CDI severity based on OTU data to
inform how the gut microbiome may modulate severity (ML-based science: good performance implies something about underlying biology), and determine whether there
is potential clinical value in OTU-based models.

# Results

## Model performance

We first set out to train the best models possible for each severity definition.
Not all samples have labels available for all four severity definitions due to
missing data for some patient lab values and incomplete chart review
(@fig-flowchart B), thus each severity definition has a different number of
samples when using as many samples as possible (@tbl-counts).
We refer to these as the full datasets.
Random forest models were trained on 100 splits of the datasets into training
and test sets, and performance was evaluated on the held-out test set using the
area under the receiver-operator characteristic curve (AUROC).
Since the severity classes are highly imbalanced with different proportions of
severe samples between definitions, we also calculated the balanced precision
and the area under the balanced precision-recall curve (AUBPRC) as first
proposed by @wu_improved_2021 to describe the precision that would be expected
if the outcome classes were balanced.

```{r model_comps_full, output = FALSE}
model_comps <- read_csv(here('results','model_comparisons.csv'))
comps_full_auroc <- model_comps %>% 
  filter(dataset == 'full', metric == 'AUROC', p_value < 0.05)
assert_that(all((comps_full_auroc %>% pull(group1)) == c("allcause", "attrib", "idsa", "idsa")))
assert_that(all((comps_full_auroc %>% pull(group2)) == c("pragmatic", "pragmatic", "allcause", "pragmatic")))
assert_that(all(
  model_comps %>%
    filter(dataset == 'full', metric == 'AUBPRC', p_value > 0.05) %>%
    select(group1, group2) %>%
    as.vector() %>% 
    unlist() == c('attrib', 'allcause')
))
```

After training on the full datasets, the performance as measured by the AUROCs
of the training set cross-validation folds were similar to those of the held-out
test sets, indicating that the models are neither overfit nor underfit
(@fig-performance A).
As measured by AUROC on the held-out test sets, models predicting pragmatic
severity performed best with a median AUROC of `r perf_medians$auroc_full_pragmatic`,
and this was significantly different from that of the other definitions 
on the full datasets (P < 0.05).
Models predicting IDSA, all-cause, and attributable severity performed similarly
with median test set AUROCs of
`r perf_medians$auroc_full_idsa`,
`r perf_medians$auroc_full_allcause`, and
`r perf_medians$auroc_full_attrib` respectively.
The test set AUROCs were not significantly different (P > 0.05) for attributable
and IDSA nor for attributable and all-cause, but the IDSA and all-cause AUROCs
were significantly different from each other (P < 0.05).
We plotted the receiver-operator characteristic curve and found that the
pragmatic severity models outperformed the others at all specificity values
(@fig-performance B).
A prior study trained a logistic regression model on whole Electronic Health
Record data extracted on the day of CDI diagnosis to predict attributable
severity, yielding an AUROC of `r ehr_auroc` [@li_using_2019].
While our OTU-based attributable severity model did not meet this performance,
the OTU-based pragmatic severity model performed just as well as the EHR-based
model in terms of AUROC.

The test set median AUBPRCs from the full datasets followed a similar pattern as 
the test set AUROCs with
`r perf_medians$aubprc_full_idsa` for IDSA severity,
`r perf_medians$aubprc_full_allcause` for all-cause severity,
`r perf_medians$aubprc_full_attrib` for attributable severity, and
`r perf_medians$aubprc_full_pragmatic` for pragmatic severity.
The AUBPRCs were significantly different from each other (P < 0.05) for
each pair of severity definitions except for attributable vs all-cause.
We plotted the balanced precision-recall curve and found that the IDSA definition
outperformed all other models at very low recall values, but the others outperform
IDSA at all other points of the curve (@fig-performance C).
The 95% confidence intervals overlapped the baseline AUROC and AUBPRC for
the attributable severity models, while all others did not overlap the baseline.

While it is advantageous to use as much data as available to train the best
models possible, comparing performances of models trained on different subsets
of the data is not entirely fair.
To enable fair comparisons of the model performances across different severity
definitions, we also selected the intersection of samples (n=`r n_cases_int`)
that had labels for all four severity definitions and repeated the model
training and evaluation process on this intersection dataset.
The attributable definition is exactly the same as the pragmatic definition for
the intersection dataset, as we defined pragmatic severity to use the
attributable label when available.
The performance results on the intersection dataset are shown in the right
facets of each panel of @fig-performance.

```{r model_comps_int, output = FALSE}

comps_int_auroc <- model_comps %>% filter(group1 != 'pragmatic', group2 != 'pragmatic') %>% 
  filter(dataset == 'int', metric == 'AUROC', p_value < 0.05)
assert_that(all(comps_int_auroc %>% pull(group1) == c('attrib', 'idsa')))
assert_that(all(comps_int_auroc %>% pull(group2) == 'allcause'))

comps_int_aubprc <- model_comps %>% filter(group1 != 'pragmatic', group2 != 'pragmatic') %>% 
  filter(dataset == 'int', metric == 'AUBPRC', p_value < 0.05)

assert_that(all(comps_int_aubprc %>% pull(group1) == c('attrib', 'idsa')))
assert_that(all(comps_int_aubprc %>% pull(group2) == 'allcause'))
```

As with the full datasets, the AUROCs of the training sets and test sets were
similar within each severity definition.
The median test set AUROCs were 
`r perf_medians$auroc_int_idsa` for IDSA severity,
`r perf_medians$auroc_int_allcause` for all-cause severity,
`r perf_medians$auroc_int_attrib` and for attributable severity.
The AUROCs on the intersection dataset were significantly different for
all-cause vs attributable and all-cause vs IDSA severity (P < 0.05), but not for
IDSA vs attributable severity (P > 0.05).
The median test set AUBPRCs were
`r perf_medians$aubprc_int_idsa` for IDSA severity,
`r perf_medians$aubprc_int_allcause` for all-cause severity,
`r perf_medians$aubprc_int_attrib` and for attributable severity.
Just as with the AUROCs, the AUBPRCs were significantly different for
all-cause vs attributable and all-cause vs IDSA severity (P < 0.05), but not for
IDSA vs attributable severity (P > 0.05).
For all severity definitions, performance dropped between the full dataset and
the intersection dataset since fewer samples are available, but this effect is
least dramatic for IDSA severity as the full and intersection datasets are more
similar for this definition.
The 95% confidence interval overlaps with the baseline for both AUROC and AUBPRC
for all definitions on the intersection dataset except for IDSA severity.

## Feature importance

We performed permutation feature importance to determine which OTUs contributed
the most to model performance (@fig-features).
An OTU was considered important if performance decreased when it was permuted in
at least 75% of the train/test splits.

## Estimating clinical value

Even if a model performs well, it may not be useful in a clinical setting unless
it can guide clinicians to choose between treatment options.
At this time, we are not aware of any direct evidence that a particular
treatment reduces the risk of severe CDI outcomes.
However, with some assumptions we offer a proof-of-concept analysis of the
potential clinical value of OTU-based severity prediction models when paired
with treatments that may reduce severity.
When considering the suitability of a model for deployment in clinical settings,
the number needed to screen (NNS) is a highly relevant metric representing how
many patients must be predicted as severe by the model to identify one true
positive.
Similarly, the number needed to treat (NNT) is the number of true positive
patients that must be treated by an intervention in order for one patient to
benefit from the treatment.
Multiplying NNS by NNT yields the number needed to benefit (NNB): the number
of patients predicted to have a severe outcome who then benefit from the
treatment [@liu_number_2019].
Thus the NNB pairs model performance with treatment effectiveness to estimate
the benefit of using predictive models in clinical practice.

Current clinical guidelines specify vancomycin and fidaxomicin as the standard
antibiotics to treat CDI, with a preference for fidaxomicin due to its higher
rate of sustained resolution of CDI and lower rate of recurrence
[@johnson_clinical_2021].
The NNTs of fidaxomicin for sustained resolution and prevention of recurrence
are each estimated to be 10 [@long_oral_2022;@tashiro_oral_2022].
However, fidaxomicin is considerably more expensive than vancomycin.
If fidaxomicin were shown to reduce the risk of severe CDI outcomes, it could be
preferentially prescribed to patients predicted to be at risk, while prescribing
vancomycin to low-risk patients.
If we assume that the superior efficacy of fidaxomicin for sustained resolution
and reduced recurrence also translates to reducing the risk of severe outcomes,
we can pair the NNT of fidaxomicin with the NNS of OTU-based prediction models
to estimate the NNB.

To calculate a clinically-relevant NNS for these models, we computed the
confusion matrix at the 95th percentile of risk for each prediction model
(@tbl-risk).
Among the models predicting severe outcomes, those trained on the full
datasets performed best with an NNS of `r allcause_nns` for the all-cause
definition, `r attrib_nns` for the attributable definition, and 
`r pragmatic_nns` for the pragmatic definition.
For context, prior studies predicted CDI-attributable severity using whole
Electronic Health Record data extracted two days after diagnosis and from a
smaller set of manually curated variables, achieving precision values of 0.417
(NNS = `r ehr_nns`) for the EHR model and 0.167 (NNS = `r curated_nns`) for the
curated model at the 95th percentile of risk
[@li_using_2019;@rao_clostridium_2015]. <!-- TODO what about NNS on day of diagnosis? -->
Multiplying the NNS of the OTU-based models by the estimated NNT of 
`r fdx_nnt` for fidaxomicin yields NNB values of `r allcause_fdx_nnb` 
for all-cause severity, `r attrib_fdx_nnb` for attributable severity,
and `r pragmatic_fdx_nnb` for pragmatic severity.
Thus, in a hypothetical scenario where these assumptions about fidaxomicin hold
true, between `r min_fdx_nnb` and `r max_fdx_nnb` patients would need to be
predicted to experience a severe outcome and be treated with fidaxomicin in
order for one patient to benefit.
As the NNS values were computed at the 95th percentile of risk (where 5% of
patients screened are predicted to experience severity), these NNB values mean
that `r min_screen` to `r max_screen` total CDI patients would need to be
screened by an OTU-based prediction model in order for one patient to benefit.
For comparison, pairing the prior EHR-based model with fidaxomicin would yield
an NNB of `r ehr_fdx_nnb` with `r ehr_screen` total CDI patients screened for
one patient to benefit.
These estimates represent a proof-of-concept demonstration of the potential
value of deploying severity prediction models to guide clinicians' treatment
decisions.

<!--
rough estimate of costs.
current: everyone gets vancomycin.
potential: patients flagged as severe get fidaxomicin. based on NNB, estimate
how much money saved in averting severe outcomes.
-->

# Discussion

Performance

Discuss important OTUs. which ones concord with literature, which ones may be new.
Abundance data are sparse, likely due to these patients being on antibiotics.
Really showcases importance of having as many samples as possible when data are
sparse and the outcome is low prevalence.

Compare to EHR-based models.

Models to guide treatment options. In the case of low-risk and non-invasive treatments
such as choosing between oral antibiotics, a higher number of false positives may
be tolerable as long as treatment cost is not unbearably high. However, for highly
invasive and irreversibly treatments such as colectomy, false positives must be minimized.
Cite studies saying fidaxomicin is cost-effective relative to vancomycin -
mentioned by @johnson_clinical_2021, e.g. @jiang_budget_2022.

It's not enough for models to perform well to justify deploying them in a
clinical setting; benefit over current practices must be shown.
Estimating the NNB contextualizes model performance within clinical reality.
Amplicon sequencing is not typically performed for CDI patients, but if there is
clinical value to be gained by implementing OTU-based models, routinely
sequencing and profiling the microbial communities of CDI patients could be
justified.

Models predicting the pragmatic definition yielded the best NNS.
While the attributable definition had a worse NNS for our OTU-based models, it
did not perform worse than the prior curated model, and it is the most
clinically relevant as physician chart review increases confidence that
positively-labelled severe outcomes are due to the CDI rather than other causes.

# Materials and Methods

## Sample collection

This study was approved by the University of Michigan Institutional Review
Board. All patient samples were collected by the University of Michigan Health
System from January 2016 through December 2017.
Stool samples that had unformed stool consistency were tested for *C. difficile* by
the clinical microbiology lab with a two-step algorithm that included detection
of *C. difficile* glutamate dehydrogenase and toxins A and B by enzyme
immunoassay with reflex to PCR for the *tcdB* gene when results were discordant.
1,517 stool samples were collected from patients diagnosed with a CDI.
Leftover stool samples that were sent to the clinical microbiology lab were
collected and split into different aliquots.
For 16S sequencing, the aliquot of stool was re-suspended in
DNA genotek stabilization buffer and then stored in the -80&deg;C freezer.
Only the first CDI sample per patient was used for subsequent ML analyses such
that no patient is represented more than once, resulting in a dataset of
`r n_cases_first` samples.

## 16S rRNA gene amplicon sequencing

Samples stored in DNA genotek buffer were thawed from the -80&deg;C, vortexed,
and then transferred to a 96-well bead beating plate for DNA extractions.
DNA was extracted using the DNeasy
Powersoil HTP 96 kit (Qiagen) and an EpMotion 5075 automated pipetting system
(Eppendorf).
The V4 region of the 16S rRNA gene was amplified with the AccuPrime
Pfx DNA polymerase (Thermo Fisher Scientific) using custom barcoded primers, as
previously described [@kozich_development_2013].
Each library preparation plate for
sequencing contained a negative control (water) and mock community control
(ZymoBIOMICS microbial community DNA standards).
The PCR amplicons were
normalized (SequalPrep normalization plate kit from Thermo Fisher Scientific),
pooled and quantified (KAPA library quantification kit from KAPA Biosystems),
and sequenced with the MiSeq system (Illumina).

All sequences were processed with mothur (v1.46) using the MiSeq SOP protocol
[@schloss_introducing_2009; @kozich_development_2013].
Paired sequencing reads were combined and
aligned with the SILVA (v132) reference database [@quast_silva_2013] and taxonomy was
assigned with a modified version of the Ribosomal Database Project reference
sequences (v16) [@cole_ribosomal_2014].
Sequences were clustered into _de novo_ OTUs with the OptiClust algorithm in
mothur [@westcott_opticlust_2017], resulting in `r num_otus` OTUs.
<!-- TODO supplementary figure with alpha and beta diversity & significance.
Samples were rarefied to 5,000 sequences per sample, repeated 1,000
times for alpha and beta diversity analysis.
 -->
 
## Defining CDI severity

We explore four different ways to define CDI cases as severe or not.
The IDSA definition of severe CDI is based on lab values collected on
the day of diagnosis, with a case being severe if serum creatinine level is
greater than or equal to $1.5 mg/dL$ and the white blood cell count is greater
than or equal to $15 k/\mu L$ [@mcdonald_clinical_2018].
The remaining definitions focus on the occurrence of adverse outcomes, which
may be more clinically relevant.
The all-cause severity definition defines a case as severe if ICU admission,
colectomy, or death occurs within 30 days of CDI diagnosis, regardless of the
cause of the adverse event.
The attributable severity definition is based on disease-related complications
defined by the CDC, where an adverse event of ICU admission, colectomy, or death
occurs within 30 days of CDI diagnosis, and the adverse event is determined to
be attributable to the CDI by physician chart review [@mcdonald_recommendations_2007].
Finally, we defined a pragmatic severity definition that makes use of the
attributable definition when available and falls back to the all-cause definition
when chart review has not been completed, allowing us to use as many samples as
we have available while taking physicians' expert opinions into account where
possible.

## Model training

Random forest models were used to examine whether OTU data collected on the day
of diagnosis could classify CDI cases as severe according to four different
definitions of severity.
We used the mikropml R package v1.5.0 [@topcuoglu_mikropml_2021] implemented in
a custom version of the mikropml Snakemake workflow [@sovacool_mikropml_2023]
for all steps of the machine learning analysis.
We have full datasets which use all samples available for each severity
definition, and an intersection dataset which consists of only the samples that
have all four definitions labelled.
The intersection dataset is the most fair for comparing model performance across
definitions, while the full dataset allows us to use as much data as possible
for model training and evaluation.
Datasets were pre-processed with the default options in mikropml to remove
features with near-zero variance and scale continuous features from -1 to 1. 
During pre-processing, `r preproc_ranges$min_removed` to `r preproc_ranges$max_removed`
features were removed due to having near-zero variance, resulting in datasets
having `r preproc_ranges$min_feats` to `r preproc_ranges$max_feats` features
depending on the severity definition.
No features had missing values and no features were perfectly correlated.
We randomly split the data into an 80% training and 20% test set and repeated
this 100 times, followed by training models with 5-fold cross-validation.

## Model evaluation

Model performance was calculated on the held-out test sets using the area under
the receiver-operator characteristic curve (AUROC) and the area under the
balanced precision-recall curve (AUBPRC).
Permutation feature importance was then performed to determine which OTUs
contributed most to model performance.
We reported OTUs with a significant permutation test in at least 75 of the 100
models.

Since the severity labels are imbalanced with different frequencies of severity
for each definition, we calculated balanced precision, the precision expected if
the labels were balanced.
The balanced precision and the area under the balanced precision-recall curve
(AUBPRC) were calculated with Equations 1 and 7 from @wu_improved_2021.

## Code availability

The complete workflow, code, and supporting files required to reproduce this
manuscript with accompanying figures is available at
<https://github.com/SchlossLab/severe-CDI>.
<!-- TODO update GitHub URL once accepted to journal -->

The workflow was defined with Snakemake [@koster_snakemake_2012] and
dependencies were managed with conda environments.
Scripts were written in R [@r_core_team_r_2020],
Python [@van_rossum_python_2009],
and GNU bash.
Additional software and packages used in the creation of this manuscript include
cowplot [@wilke_cowplot_2020],
ggtext [@wilke_ggtext_2020],
ggsankey [@sjoberg_ggsankey_2022],
schtools [@sovacool_schtools_2022],
the tidyverse metapackage [@wickham_welcome_2019],
Quarto, and
vegan [@oksanen_vegan_2023].

## Data availability

The 16S rRNA sequencing data have been deposited in the National Center for
Biotechnology Information Sequence Read Archive
(BioProject Accession no. PRJNA729511).

<!--
# Acknowledgements

TODO funding & author contributions
-->

{{< pagebreak >}}

# References {.unnumbered}

::: {#refs}
:::

# Tables
{{< pagebreak >}}

```{r sample_counts}
#| label: tbl-counts
#| tbl-pos: h
#| tbl-cap: "**Sample counts and proportion of severe cases.** Each severity 
#|   definition has a different number of patient samples available, as well as
#|   a different proportion of cases labelled as severe.
#| "
#| tbl-subcap: 
#|   - "Full datasets"
#|   - "Intersection of samples with all labels available"
#| layout-ncol: 2
format_table <- function(tab) {
  tab %>%
    kable(col.names = c(' ', 'IDSA', 'Attributable', 'All-cause', "Pragmatic"),
          format.args = list(big.mark = ',')) %>% 
    kable_styling(font_size = table_font_size)
}
read_csv(here('results','count_table_full.csv')) %>% 
  format_table()
read_csv(here('results','count_table_int.csv')) %>% 
  format_table()
```


```{r risk_95th_pct_decision_thresholds, message=FALSE}
#| label: tbl-risk
#| tbl-pos: h
#| tbl-cap: "**Predictive model performance at 95th percentile of risk.** 
#|   The confusion matrix was computed for the decision threshold at
#|   the 95th percentile of risk for each severity prediction model, 
#|   which corresponds to 5% of cases predicted to have a severe outcome. 
#|   The number needed to screen (NNS) to identify one true positive
#|   is the reciprocal of precision.
#|   "
#| tbl-subcap: 
#|   - "Full datasets"
#|   - "Intersection of samples with all labels available"
risk_thresh <- read_csv(here('results','decision_thresholds.csv'))
risk_thresh %>% 
  filter(Dataset == 'Full') %>% 
  select(-Dataset) %>% 
  kable() %>% 
  kable_styling(font_size = table_font_size)
risk_thresh %>% 
  filter(Dataset == 'Intersection') %>% 
  select(-Dataset) %>% 
  kable() %>% 
  kable_styling(font_size = table_font_size)
```


#  Figures

::: {#fig-flowchart fig-cap="**CDI severity definitions.**
**A)** Decision flow chart to define CDI cases as severe according to the
Infectious Diseases Society of America (IDSA) based on lab values,
the occurrence of an adverse outcome due to any cause (All-cause),
and the occurrence of disease-related complications confirmed as attributable to
CDI with chart review (Attributable).
**B)** The proportion of severe CDI cases labelled according to each definition.
An additional 'Pragmatic' severity definition uses the Attributable definition
when possible, and falls back to the All-cause definition when chart review is
not available.
<!-- TODO table (supplementary?) showing counts & frequency of positives-->
"}
![](figures/flowchart_sankey.png)
:::

::: {#fig-performance fig-cap="**Performance of ML models.**
In the left facets, models were trained on the full datasets, with different
numbers of samples available for each severity definition.
In the right facets, models were trained on the same dataset consisting of the
intersection of samples with all labels available for each definition.
Note that the intersection dataset for Attributable and Pragmatic severity have
exactly the same labels, thus they have identical performance.
**A)** 
Area under the receiver-operator characteristic curve (AUROC) for the test sets
and cross-validation folds of the training sets, and the area under the balanced
precision-recall curve (AUBPRC) for the test sets.
Each point is the median performance across 100 train/test splits with tails as
the 95% CI.
Nearly all pairs of definitions have significantly different performances on the
test set (P < 0.05) except for AUROC and AUBPRC of Attributable vs. Pragmatic on
the intersection dataset (as they are identical), AUROC of Attributable vs.
All-cause on the full dataset, and AUROC of Attributable vs. IDSA on the full
dataset.
**B)** Receiver-operator characteristic curves for the test sets.
Mean specificity is reported at each sensitivity value, 
with ribbons as the 95% CI.
**C)** Balanced precision-recall curves for the test sets.
Mean balanced precision is reported at each recall value, 
with ribbons as the 95% CI.
"}
![](figures/ml-performance.png)
:::

::: {#fig-features fig-cap="**Feature importance.**
**A)** Feature importance via permutation test.
For each OTU, the order of samples was randomized in the test set 100 times and
the performance was re-calculated to estimate the permutation performance.
An OTU was considered important if the performance decreased when the OTU was
permuted in at least 75% of the models.
OTUs with a greater difference in AUROC (actual performance minus permutation
performance) are more important.
Left: models were trained on the full datasets, with different numbers of
samples available for each severity definition.
Right: models were trained on the intersection of samples with all labels
available for each definition. Note that Attributable and Pragmatic severity are
exactly the same for the intersection dataset.
*Pseudomonas* (OTU 120) is not shown for IDSA severity in the full datasets nor
in the intersection dataset because it was removed during pre-processing due to
having near-zero variance.
**B)** Log~10~-transformed median relative abundances of the most important OTUs
on the full datasets, grouped by severity (shape). 
The vertical dashed line is the limit of detection.
"}
![](figures/feature-importance.png)
:::
