---
title: "Predicting _C. difficile_ Infection Severity from the Taxonomic Composition of the Gut Microbiome"
bibliography: references.bib
fontsize: 11pt
date: last-modified
date-format: medium
format:
  manuscript-pdf:
    mainfont: Helvetica
    keep-tex: true
    linenumbers: true
    doublespacing: true
    lof: false
  html:
    embed-resources: true
runningtitle: Predicting CDI Severity from OTUs
runningauthor: Sovacool
papertype: Preprint
author:
  - name: Kelly L. Sovacool
    orcid: 0000-0003-3283-829X
    affiliations:
      - ref: bioinf
  - name: Sarah E. Tomkovich
    affiliations:
      - ref: micro
  - name: Megan L. Coden
    affiliations:
      - ref: mcdb
  - name: Vincent B. Young
    affiliations:
      - ref: micro
      - ref: intmed
  - name: Krishna Rao
    affiliations:
      - ref: intmed
  - name: Patrick D. Schloss
    orcid: 0000-0002-6935-4275
    email: pschloss@umich.edu
    corresponding: true
    affiliations:
      - ref: micro
      - ref: ccmb
affiliations:
  - id: bioinf
    name: Department of Computational Medicine & Bioinformatics, University of Michigan
  - id: micro
    name: Department of Microbiology & Immunology, University of Michigan
  - id: mcdb
    name: Department of Molecular, Cellular, and Developmental Biology, University of Michigan
  - id: intmed
    name: Division of Infectious Diseases, Department of Internal Medicine, University of Michigan
  - id: ccmb
    name: Center for Computational Medicine and Bioinformatics, University of Michigan
abstract: TODO
importance: TODO
keywords: _C. difficile_ infection, supervised machine learning, gut microbiome, amplicion sequencing
---

```{r load_data}
library(here)
library(tidyverse)

load(here("results", "stats.RData"))
```

# Introduction

A few ways to define CDI severity (@fig-flowchart)

The IDSA definition is known to be a poor predictor of adverse outcomes [@stevens_validation_2020],
however, it is easy to collect.

# Results

## Model performance

Report median AUROC for training set and test set, and median AUBPRC for test set (@fig-performance).

## Feature importance

Most important OTUs contributing to model performance (@fig-features)

## Clinical value of severity prediction models

# Discussion

TODO

# Materials and Methods

## Sample collection

This study was approved by the University of Michigan Institutional Review
Board. All patient samples were collected by the University of Michigan Health
System from January 2016 through December 2017.
Stool samples that had unformed stool consistency were tested for *C. difficile* by
the clinical microbiology lab with a two-step algorithm that included detection
of *C. difficile* glutamate dehydrogenase and toxins A and B by enzyme
immunoassay with reflex to PCR for the *tcdB* gene when results were discordant.
1,517 stool samples were collected from patients diagnosed with a CDI.
Leftover stool samples that were sent to the clinical microbiology lab were
collected and split into different aliquots.
For 16S sequencing, the aliquot of stool was resuspended in
DNA genotek stabilization buffer and then stored in the -80&deg;C freezer.
Only the first CDI sample per patient was used for subsequent ML analyses such
that no patient is represented more than once, resulting in a dataset of
`r n_cases_first` samples.

## 16S rRNA gene amplicon sequencing

Samples stored in DNA genotek buffer were thawed from the -80&deg;C, vortexed,
and then transferred to a 96-well bead beating plate for DNA extractions.
DNA was extracted using the DNeasy
Powersoil HTP 96 kit (Qiagen) and an EpMotion 5075 automated pipetting system
(Eppendorf).
The V4 region of the 16S rRNA gene was amplified with the AccuPrime
Pfx DNA polymerase (Thermo Fisher Scientific) using custom barcoded primers, as
previously described [@kozich_development_2013].
Each library preparation plate for
sequencing contained a negative control (water) and mock community control
(ZymoBIOMICS microbial community DNA standards).
The PCR amplicons were
normalized (SequalPrep normalizatin plate kit from Thermo Fisher Scientific),
pooled and quantified (KAPA library quantification kit from KAPA Biosystems),
and sequenced with the MiSeq system (Illumina).

All sequences were processed with mothur (v1.46) using the MiSeq SOP protocol
[@schloss_introducing_2009; @kozich_development_2013].
Paired sequencing reads were combined and
aligned with the SILVA (v132) reference database [@quast_silva_2013] and taxonomy was
assigned with a modified version of the Ribosomal Database Project reference
sequences (v16) [@cole_ribosomal_2014].
Sequences were clustered into _de novo_ OTUs with the OptiClust algorithm in
mothur [@westcott_opticlust_2017], resulting in 
Samples were rarefied to 5,000 sequences per sample, repeated 1,000
times for alpha and beta diversity analysis.
<!-- TODO supplementary figure with alpha and beta diversity & significance -->

## Defining CDI severity

We explore four different ways to define CDI cases as severe or not.
The IDSA definition of severe CDI is based on lab values collected on
the day of diagnosis, with a case being severe if serum creatinine level is
greater than or equal to $1.5 mg/dL$ and the white blood cell count is greater
than or equal to $15 k/\mu L$ [@mcdonald_clinical_2018].
The remaining definitions focus on the occurrence of adverse outcomes, which
may be more clinically relevant.
The all-cause severity definition defines a case as severe if ICU admission,
colectomy, or death occurs within 30 days of CDI diagnosis, regardless of the
cause of the adverse event.
The attributable severity definition is based on disease-related complications
defined by the CDC, where an adverse event of ICU admission, colectomy, or death
occurs within 30 days of CDI diagnosis, and the adverse event is determined to
be attributable to the CDI by physician chart review [@mcdonald_recommendations_2007].
Finally, we defined a pragmatic severity definition that makes use of the
attributable definition when available and falls back to the all-cause definition
when chart review has not been completed, allowing us to use as many samples as
we have available while taking physicians' expert opinions into account where
possible.

## Model training and evaluation

Random forest models were used to examine whether OTU data collected on the day
of diagnosis could classify CDI cases as severe according to four different
definitions of severity.
We used the mikropml R package v1.5.0 [@topcuoglu_mikropml_2021] implemented in
a custom version of the mikropml Snakemake workflow [@sovacool_mikropml_2023]
for all steps of the machine learning analysis.
We have a full dataset which uses all samples available for each severity
definition, and an intersection dataset which consists of only the samples that
have all four definitions labelled.
The intersection dataset is the most fair for comparing model performance across
definitions, while the full dataset allows us to use as much data as possible
for model training and evaluation.
Datasets were preprocessed with the default options in mikropml to remove
features with near-zero variance and scale continuous features from -1 to 1. <!-- No features had missing values and no features were perfectly correlated -->
During preprocessing, `r preproc_ranges$min_removed` to `r preproc_ranges$max_removed`
features were removed due to having near-zero variance, resulting in datasets
ranging from `r preproc_ranges$min_feats` to `r preproc_ranges$max_feats`
depending on the severity definition.
We randomly split the data into an 80% training and 20% test set and repeated
this 100 times, followed by training models with 5-fold cross-validation.
Model performance was calculated on the test set using the area under the
receiver-operator characteristic curve (AUROC) and the area under the balanced
precision-recall curve (AUBPRC).
Permutation feature importance was then performed to determine which OTUs
contributed most to model performance.
We reported OTUs with a significant permutation test in at least 80 of the 100
models.

Since the severity labels are imbalanced with different frequencies of severity
for each definition, we calculated balanced precision, the precision expected if
the labels were balanced.
The balanced precision and the area under the balanced precision-recall curve
(AUBPRC) were calculated with Equations 1 and 7 from @wu_improved_2021.

## Code availability

The complete workflow, code, and supporting files required to reproduce this
manuscript with accompanying figures is available at
<https://github.com/SchlossLab/severe-CDI>.
<!-- TODO update GitHub URL once accepted to journal -->

The workflow was defined with Snakemake [@koster_snakemake_2012] and
dependencies were managed with conda environments.
Scripts were written in R [@r_core_team_r_2020],
Python [@van_rossum_python_2009],
and GNU bash.
Additional software and packages used in the creation of this manuscript include
cowplot [@wilke_cowplot_2020],
ggtext [@wilke_ggtext_2020],
ggsankey [@sjoberg_ggsankey_2022],
schtools [@sovacool_schtools_2022],
the tidyverse metapackage [@wickham_welcome_2019],
Quarto, and
vegan [@oksanen_vegan_2023].

## Data availability

The 16S rRNA sequencing data have been deposited in the National Center for
Biotechnology Information Sequence Read Archive
(BioProject Accession no. PRJNA729511).

# Acknowledgements

TODO

# References {.unnumbered}

::: {#refs}
:::

# Figures

::: {#fig-flowchart fig-cap="**CDI severity definitions.**
**A)** Decision flow chart to define CDI cases as severe according to the Infectious
Diseases Society of America (IDSA) based on lab values,
the occurence of an adverse outcome due to any cause (All-cause),
and the occurence of disease-related complications confirmed as attributable to
CDI with chart review (Attrib).
**B)** The proportion of severe CDI cases labelled according to each definition.
An additional 'Pragmatic' severity definition uses the Attributable definition
when possible, and falls back to the All-cause definition when chart review is
not available.
<!-- TODO table (supplementary?) showing counts & frequency of positives-->
"}
![](figures/flowchart_sankey.png)
:::

::: {#fig-performance fig-cap="**Performance of ML models.**
Area under the receiver-operator characteristic curve (AUROC) for the test sets
and cross-validation folds of the training sets,
and the area under the balanced precision-recall curve (AUBPRC) for the test sets.
Left: models were trained on the full dataset, with different numbers of samples
available for each severity definition.
Right: models were trained on the intersection of samples with all labels
available for each definition. Note that Attributable and Pragmatic severity are
exactly the same for the intersection set.
<!-- TODO add plots of AUROC and AUBPRC curves -->
"}
![](figures/ml-performance.png)
:::

::: {#fig-features fig-cap="**Feature importance.**
Feature importance via permutation test.
For each OTU, the order of samples was randomized in the test set 100 times and
the performance was re-calculated to estimate the permutation performance.
An OTU was considered important if the performance decreased when the OTU was
permuted in at least 75% of the models.
OTUs with a greater difference in AUROC (actual performance minus permutation
performance) are more important.
Left: models were trained on the full dataset, with different numbers of samples
available for each severity definition.
Right: models were trained on the intersection of samples with all labels
available for each definition. Note that Attributable and Pragmatic severity are
exactly the same for the intersection set.
OTU 120 (*Pseudomonas*) is not shown for the full data set with IDSA severity 
on the full dataset because it was removed during pre-processing due to having
near-zero variance.
"}
![](figures/feature-importance.png)
:::
